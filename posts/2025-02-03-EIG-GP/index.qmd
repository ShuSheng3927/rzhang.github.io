---
title: "Expected Information Gain with Gaussian Process Surrogate Models"
description: "Computations and derivations of the expected information gain utility function of active learning when the surrogate model is a conjugate Gaussian process."
date: "03 February 2025"
bibliography: references.bib
categories:
  - Active Learning
  - Gaussian Process
---

### Expected Information Gain 

In Bayesian experiment design [@rainforth2024modern], a commonly used utility function is the information gain, where we are comparing the entropy of the distributions before and after observing an addition point. Assuming that our existing dataset is denoted by $\mathcal{D}$ and the posterior distribution is $p(\cdot | \mathcal{D})$. If we make an observation at $x$ and observe $y$, our new dataset will become $\mathcal{D}^+ := \mathcal{D} \cup \{(x, y)\}$. This will then correspond to a new posterior $p(\cdot | \mathcal{D}^+)$. 

Given those, the **information gain** (IG) is given by:

$$
IG(x) = H(p(\cdot | \mathcal{D})) - H(p(\cdot | \mathcal{D}^+)).
$$

### Gaussian Process Posteriors

Consider our distribution is a Gaussian process (GP) with mean zero and kernel $k$, and the posterior is the posterior predictive distribution of this GP on some finite set of test points $x_*$ with size $m$. We also assume the current dataset $\mathcal{D} := \{(x_i, y_i)\}_{i=1}^{n}$ is of size $n$ and the observations with additive, centered, independent Gaussian noise of variance $\sigma^2$. 

We will also use the following notations to denote the various Gram matrices using kernel $k$

- $K = k(X,X)$, size $n \times n$.
- $K_* = k(X, x_*)$, size $n \times m$. 
- $K_{**} = k(x_*, x_*)$, size $m \times m$. 

The posterior is therefore (see [here](https://shusheng3927.github.io/posts/2024-10-13-basic-GP-regression-formula/) for a detailed derivation)

$$
\begin{split}
p(y^* | x^*, \mathcal{D}, \sigma^2) &\sim \mathcal{N}(\mu_{y^*|\mathcal{D}}, \Sigma_{y^*|\mathcal{D}}) \\
&\mu_{y^*|\mathcal{D}} = K_*^T (K + \sigma^2 I_n)^{-1} y\\
&\Sigma_{y^*|\mathcal{D}} = K_{**} - K_*^T (K + \sigma^2 I_n)^{-1} K_*.
\end{split}
$$

After adding a new observation at $x$, we will have an updated dataset $\mathcal{D}^+$ with $X^+ = X \cup \{x\}$ and have an updated posterior using the following Gram matrices

- $K^+ = k(X^+,X^+)$, size $(n+1) \times (n+1)$.
- $K_*^+ = k(X^+, x_*)$, size $(n+1) \times m$. 
- $K_{**}^+ = K_{**} = k(x_*, x_*)$, size $m \times m$.

So, the updated posterior's covariance matrix is

$$
\Sigma_{y^*|\mathcal{D}^+} = K_{**}^+ - K_*^{+T} (K^+ + \sigma^2 I_{n+1})^{-1} K_*^+
$$

### Information Gain Calculation

The information gain can be written as 

$$
IG(x) = H(p(\cdot | \mathcal{D})) - H(p(\cdot | \mathcal{D}^+))
$$
where using the [definition of the entropy of multivariate Gaussian](https://statproofbook.github.io/P/mvn-dent) yields

$$
\begin{split}
IG(x) &= \frac{1}{2} \log \det \Sigma_{y^*|\mathcal{D}} - \frac{1}{2} \log \det \Sigma_{y^*|\mathcal{D}^+} \\
&= \frac{1}{2} \log \det \Big( K_{**} - K_*^T (K + \delta^2 I_n)^{-1} K_* \Big) - \frac{1}{2} \log \det \Big( K_{**}^+ - K_*^{+T} (K^+ + \sigma^2 I_{n+1})^{-1} K_*^+\Big)
\end{split}
$$
Since $IG(x)$ is independent of $y | x$, the acquisition function **expected information gain** (EIG) is therefore 

$$
EIG(x) = \mathbb{E}_{y}[IG(x)] = IG(x)
$$
Furthermore, we can remove several terms when we do $\arg\max_x$ for the acquisition function optimisation, and get 

$$
EIG(x) = - \log \det \Big( K_{**}^+ - K_*^{+T} (K^+ + \sigma^2 I_{n+1})^{-1} K_*^+\Big).
$$

### Computational Tricks

We will consider the naive computation of the above $EIG(x)$ expression. One should note that in the active learning settings, we would often be in the scenarios where $m >> n$. An improved approach of computing the same quantity is presented below, leveraging the matrix determinant lemma. 

#### Naive Implementation

We wish to compute

$$
EIG(x) = - \log\det \Big( K_{**}^+ - K_*^{+T} (K^+ + \sigma^2 I_{n+1})^{-1} K_*^+\Big).
$$

| Order | Expression  | Cost |
| - | ----- | -- |
| 1  | $(K^+ + \delta^2 I_{n+1})^{-1}$ | $O((n+1)^3)$ |
| 2  | $(K^+ + \delta^2 I_{n+1})^{-1} K_*^+$ | $O((n+1)^2 m)$ |
| 3  | $K_*^{+T} (K^+ + \delta^2 I_{n+1})^{-1}K_*^+$ | $O(m (n+1)^2)$ |
| 4  | $K^+ - K_*^{+T} (K^+ + \delta^2 I_{n+1})^{-1} K_*^+$ | $O(m^2)$ |
| 5  | $-\log\det\big(K_{**}^+ - K_*^{+T} (K^+ + \delta^2 I_{n+1})^{-1} K_*^+ \big)$ | $O({\color{red}m^3})$ |

So the cost is

$$
O((n+1)^3 + (n+1)^2 m + m^2 (n+1) + m^2 + {\color{red}m^3}).
$$
We will need to compute the above quantity $m$ times for comparison $\arg\max_x$, thus the full costs is 

$$
O((n+1)^3m + (n+1)^2 m^2 + m^3 (n+1) + m^3 + {\color{red}m^4}).
$$
#### Nontrivial Implementation

We use the [matrix determinant identity](https://en.wikipedia.org/wiki/Matrix_determinant_lemma):

$$
\det(A + UWV^T) = \det(A) \det(W) \det(W + V^T A^{-1} U)
$$

where here

- $A = K_{**}^+$
- $U = -K_*^{+T}$
- $W = (K^+ + \sigma^2 I_{n+1})^{-1}$
- $V = K_*^+$

Thus, we wish to compute

$$
EIG(x) = -\log \left[ \det(K_{**}^+) \cdot 1/ \det(K^+ + \sigma^2 I_{n+1}) \cdot \det \big(K^+ + \sigma^2 I_{n+1}- K_*^+ (K_{**}^+)^{-1}K_*^{+T} \big) \right]
$$

Since $K_{**}$ is positive semi-definite, its determinant is always non-negative so we can ignore it in comparisons.

| Order | Expression  | Cost |
| - | ------ | -- |
| 1  | $K_{**}^+ + \sigma^2 I_{n+1}$ | $O((n+1)^2)$ |
| 2  | $\det(K^+ + \sigma^2 I_{n+1})$ | $O((n+1)^3)$ |
| 3  | $(K_{**}^+)^{-1}$ | $O(m^3)$, reusable |
| 4  | $(K_{**}^+)^{-1}K_*^{+T}$ | $O(m^2(n+1))$ |
| 5  | $K_*^+ (K_{**}^+)^{-1}K_*^{+T}$ | $O(m(n+1)^2)$ |
| 6  | $(K^+ + \sigma^2 I_{n+1}) - K_*^+ (K_{**}^+)^{-1}K_*^{+T}$ | $O((n+1)^2)$ |
| 7  | $\det\big( (K^+ + \sigma^2 I_{n+1}) - K_*^+ (K_{**}^+)^{-1}K_*^{+T} \big)$ | $O((n+1)^3)$ |
| 8  | $\log \left[ 1/ \det(K^+ + \sigma^2 I_{n+1}) \cdot \det \big(K^+ + \sigma^2 I_{n+1}- K_*^+ (K_{**}^+)^{-1}K_*^{+T} \big) \right]$ | $O(1)$ |

So the cost is
$$
O((n+1)^2 + (n+1)^3 + {\color{blue}m^3} + m^2(n+1) + m(n+1)^2).
$$

We will need to compute the above quantity $m$ times for comparison $\arg\max_x$, thus the full costs is 

$$
O((n+1)^2m + (n+1)^3m + {\color{blue}m^3} + m^3(n+1) + m^2(n+1)^2).
$$