{
  "hash": "8110de5a1e2926333b567651b2492ce9",
  "result": {
    "markdown": "---\ntitle: \"Spatial-Temporal GP (2)\"\ndescription: \"A series of blog posts on spatial-temporal Gaussian processes. Exploiting the Kronecker structure of temporal GP regression with 1d space.\"\ndate: \"31 October 2024\"\ncategories:\n  - Gaussian Process\n---\n\n\n\\newcommand{\\vec}{\\operatorname{vec}}\n\nIn this blog post, I will walk through how one could exploit the Kronecker structure of the temporal Gaussian process (GP) regression with one-dimensional space + one-dimensional time inputs and one-dimensional output. This is the second of a series of blog posts on spatial-temporal Gaussian processes. \n\n### Separable Kernels\n\nRecall from the [last post](https://shusheng3927.github.io/posts/2024-10-23-temporal-GP-1/) that we have fitted a temporal GP on an one-dimensional spatial and one-dimensional temporal grid. Since we define the overall kernel as a product of the spatial and temporal component of the kernel, i.e. $k = k_s \\times k_t$, we have the **Kronecker structure** of the Gram matrices $K = K_s \\otimes K_t$, visually shown below. \n\n![Gram Matrices](./gram_matrix_krnonecker_1d.png)\n\nSuch kernels are known as **separable** kernels, and in this post we will explore how one could exploit this structure to obtain significant computational speed ups. \n\n### Kronecker Facts\n\nBefore describing how one could leverage the Kronecker structure, first we state several relevant and helpful facts about matrices with a [Kronecker structure](https://en.wikipedia.org/wiki/Kronecker_product). \n\nConsider two matrices $A \\in \\mathbb{R}^{n_1 \\times n_2}$, $B \\in \\mathbb{R}^{m_1 \\times m_2}$. The Kronecker product $K = A \\otimes B \\in \\mathbb{R}^{n_1 m_1 \\times n_2 m_2}$ is defined by \n\n$$\nA \\otimes B = \\begin{bmatrix}\na_{11} B & \\cdots & a_{1 n_2} B \\\\\n\\vdots & \\ddots & \\vdots \\\\\na_{n_1 1} B & \\cdots & a_{n_1 n_2} B \n\\end{bmatrix}.\n$$\n\nThe Kronecker product operator $\\otimes$ is bi-linear and associative, so we have \n\n$$\n\\begin{split}\nA \\otimes (B+C) &= A \\otimes B + A \\otimes C \\\\\n(B+C) \\otimes A &= B \\otimes A + C \\otimes A \\\\\n(k A) \\otimes B &= A \\otimes (k B) = k (A \\otimes B) \\\\\nA \\otimes (B \\otimes C) &= (A \\otimes B ) \\otimes C. \\\\\n\\end{split}\n$$\nMore interesting (and relevant here) properties are the ones related to inverse, Cholesky decomposition, and determinant.\n\nFirst, we have the <u>inverse property</u>\n\n$$\n(A \\otimes B)^{-1} = A^{-1} \\otimes B^{-1}\n$$\nfor any invertible $A, B$. \n\nNext, we have the <u>mixed-product property</u>\n\n$$\n(A_1 \\otimes B_1) (A_2 \\otimes B_2) = (A_1 A_2) \\otimes (B_1 B_2).\n$$\nNote that if we have <u>Cholesky decomposition</u> $A = LL^*$ for lower triangular matrix $L$ and its conjugate transpose $L^*$, we have \n\n$$\nA \\otimes B = (L_A L_A^*) \\otimes (L_B L_B^*) = (L_A \\otimes L_B) (L_A^* \\otimes L_B^*).\n$$\nSimilarly, if we have <u>eigendecomposition</u> $A = Q_A \\Lambda_A Q_A^T$ for diagonal matrix $\\Lambda_A$, we have \n\n$$\nA \\otimes B = (Q_A \\Lambda_A Q_A^T) \\otimes (Q_B \\Lambda_B Q_B^T) = (Q_A \\otimes Q_B) (\\Lambda_A \\otimes \\Lambda_B) (Q_A^T \\otimes Q_B^T).\n$$\n\nFinally, if we have square matrices $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{m \\times m}$, then \n\n$$\n| A \\otimes B | = | A |^m | B |^n\n$$\nfor matrix <u>determinants</u>. \n\n#### Kronecker Matrix-Vector Product\n\nWe will first show a useful algorithm to compute matrix-vector product of the form $(A \\otimes B) z$ where $A, B$ are square matrices of size $N_A \\times N_A$ and $N_B \\times N_B$ respectively and $z \\in \\mathbb{R}^{N}$ with $N = N_A N_B$. Note that this algorithm can be generalised to matrix-vector product with matrix being a Kronecker product of $D$ square matrices. \n\nWe will use the $\\vec$ operator where it stacks the columns of a matrix vertically to obtain a single column vector, i.e. for $A = [a_1, a_2, \\ldots, a_k]$ with $a_i$ being column vectors, we have \n\n$$\n\\vec(A) = \\begin{bmatrix}\na_1 \\\\\n\\vdots \\\\\na_k\n\\end{bmatrix}.\n$$\n\nA property about the $\\vec$ operator and Kronecker product is the following: \n\n$$\n(A \\otimes B) \\vec(z) = \\vec [B z A^T] = \\vec [B (A z^T)^T]\n$$\nBack to the product of interest $(A \\otimes B) z$, we have $z$ as a column vector. To apply the $\\vec$ formula above, we need to reshape $z$ to enable sensible matrix products. So, we have, using JAX (and JAX Numpy) notations, \n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep1 = z.reshape(N_A, N_B)\nstep2 = A @ step1\nstep3 = step2.T\nstep4 = B @ step3\nresult = step4.T.flatten()\n```\n:::\n\n\nwhere the `.reshape` in JAX Numpy is practically transpose then reshape - which is also why we transpose before flatten to get the final result. \nIn terms of computational time, the naive implementation of $(A \\otimes B) z$ will be $O( (N_A N_B)^2)$ whereas the Kronecker implementation is only $O(N_A N_B)$. The Kronecker implementation will be used whenever it is applicable. \n\n#### Kronecker Matrix-Matrix Product\n\nOne could also easily extend the above Kronecker matrix-vector product to Kronecker matrix-matrix product in the following way. Consider the matrix-matrix product $(A \\otimes B) Z$ where $A, B$ are square matrices of size $N_A \\times N_A$ and $N_B \\times N_B$ respectively and $Z \\in \\mathbb{R}^{N \\times M}$. We will break matrix $Z$ down in to $M$ columns and perform Kronecker matrix-vector product to each of the columns. This gives a computational time of $O(N_A N_B M)$ as opposed to the $(N_A N_B)^2M$ of naive implementation. We could also exploit the vectorisation functionalities to further speed up this product using methods such as `jax.vmap`.\n\n### Standard GP Sampling, Training, and Prediction\n\nFor a Gaussian process $f \\sim \\mathcal{GP}(\\mu, k)$ where $\\mu$ is the mean function and $k$ is the kernel function, we can draw a <u>sample</u> of $f$ at test locations $X_* = (x_1^*, x_2^*, \\ldots, x_k^*)$ \n\n$$\nf_* = \\mu(X_*) + \\sqrt{k(X_*, X_*)} ~\\xi, \\qquad \\xi \\sim N_k(0, I_k) \n$$\nwhere $k(X_*, X_*)$ is the Gram matrix and the square root denotes the lower Cholesky factor. \n\nConsider we have made $m$ observations of this GP $f$ where the observations are made at locations $X \\in \\mathbb{R}^m$ with values $y \\in \\mathbb{R}^m$ and the observations are noisy with independent additive Gaussian noise of variance $\\sigma^2$, i.e. $y = f(X) + \\xi$ with $\\xi_i \\sim N(0, \\sigma^2) ~\\forall i = 1, 2, \\ldots, m$. Denote the existing observations as $\\mathcal{D} = \\{ X, y \\}$.\n\nTo train the model using data (or conduct MLE), we need to optimise the <u>log likelihood</u> \n\n$$\n\\log p(y|X) = - \\frac{m}{2}\\log(2\\pi) - \\log | k(X, X) + \\sigma^2 I_m | - \\frac{1}{2} y ^T ( k(X, X) + \\sigma^2 I_m)^{-1} y .\n$$\nIn addition, we have the <u>predictive</u> distribution \n\n$$\n\\begin{split} \ny_* ~|X_*, \\mathcal{D}, \\sigma^2 &\\sim N_{n}(\\mu_{y_* | \\mathcal{D}}, K_{y_* | \\mathcal{D}}), \\\\\n\\mu_{y_* | \\mathcal{D}} &= K_*^T (K + \\sigma^2 I_n)^{-1} y,\\\\\nK_{y_* | \\mathcal{D}} &= K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1}K_*.\n\\end{split}\n$$\n\nwhich also implies that if we wish to draw a posterior sample we would have \n\n$$\nf_* = \\mu_{y_* | \\mathcal{D}}(X_*) + \\sqrt{k_{y_* | \\mathcal{D}}(X_*, X_*)} ~\\xi, \\qquad \\xi \\sim N_k(0, I_k). \n$$\n\n### \"GP Does Not Scale\"\n\nIt is a consensus / folklore that GP does not scale. This is mostly due to the fact that the training and sampling of GP involves inversions and Cholesky decomposition of $m \\times m$ matrices where $m$ is the number of observations. Most commonly used algorithms for matrix inversion and Cholesky decomposition are of $O(m^3)$ time complexity and are serial in natural (so do not enjoy the GPU speed-ups that are prevalent in machine learning) -- even a moderately sized data set will induce prohibitive costs. \n\nIt is still ongoing research to device tricks and algorithms to make GP more scalable. Some notable approaches includes: \n\n- [Inducing Points](https://arxiv.org/abs/2012.13962)\n- [Vecchia Approximations](https://arxiv.org/abs/1708.06302)\n- [SPDE Approach](https://arxiv.org/abs/2111.01084)\n- [Efficiently Posterior Sampling](https://arxiv.org/abs/2002.09309)\n- [Conjugate Gradients](https://geoffpleiss.com/static/media/gpleiss_thesis.d218bc00.pdf).\n\nHere, we will look at one such approach: Kronecker structure exploiting method. Assume we have a 1D space + 1D time temporal GP with $N_s$ spatial grid points and $N_t$ temporal grid points. The naive implementation will have a time complexity of $O((N_s N_t)^3)$, whereas a Kronecker-aware implementation will only have a time complexity of $O(\\max\\{N_s, N_t\\}^3)$. Below, we will clarify the precise ways we can leverage the matrix structure to achieve computational speed-ups. \n\n### Kronecker Product Gaussian Process\n\n*The contents here are largely based on [Saatchi (2011)](https://mlg.eng.cam.ac.uk/pub/pdf/Saa11.pdf).*\n\n#### Sampling from a GP\n\n**Naive**\n\n$$\nf_* = \\mu(X_*) + \\sqrt{K_s \\otimes K_t}  ~\\xi, \\qquad \\xi \\sim N_k(0, I_k) \n$$\n\n**Kronecker**\n\n$$\nf_* = \\mu(X_*) + \\left( \\sqrt{K_s} \\otimes \\sqrt{K_t} \\right) ~\\xi, \\qquad \\xi \\sim N_k(0, I_k) \n$$\n\nwhere we can use the Kronecker matrix-vector product. \n\n#### GP Likelihood\n\n**Naive**\n\n$$\n\\log p(y|X) = - \\frac{m}{2}\\log(2\\pi) - \\log | K_s \\otimes K_t + \\sigma^2 I_m | - \\frac{1}{2}  y^T ( K_s \\otimes K_t + \\sigma^2 I_m)^{-1}y.\n$$\n\n**Kronecker**\n\nThere are two places where we need to leverage the Kronecker structure: \n\n- determinant $| K_s \\otimes K_t + \\sigma^2 I_m |$\n- inverse $( K_s \\otimes K_t + \\sigma^2 I_m )^{-1}$.\n\nConsider eigendecompositions $K = Q \\Lambda Q^T$, $K_s = Q_s \\Lambda_s Q_s^T$, and $K_t = Q_t \\Lambda_t Q_t^T$. We know that $QQ^T = I$ and $|Q|=1$, so since\n\n$$\n\\begin{split}\nK + \\sigma^2 I_m  &= Q \\Lambda Q^T + Q (\\sigma^2 I_m) Q^T = Q (\\Lambda + \\sigma^2 I_m) Q^T \\\\\nK_s \\otimes K_t + \\sigma^2 I_m &= (Q_s \\otimes Q_t) (\\Lambda_s \\otimes \\Lambda_t + \\sigma^2 I_m) (Q_s^T \\otimes Q_t^T)\n\\end{split}\n$$\n\nwe have \n\n$$\n\\begin{split}\n| K + \\sigma^2 I_m | &= |Q| \\cdot |\\Lambda + \\sigma^2 I_m| \\cdot |Q^T| = |\\Lambda_s \\otimes \\Lambda_t + \\sigma^2 I_m| \\\\\n( K + \\sigma^2 I_m)^{-1} &= Q^{-T} (\\Lambda + \\sigma^2 I_m)^{-1} Q^{-1} = Q (\\Lambda_s \\otimes \\Lambda_t + \\sigma^2 I_m)^{-1} Q^{T}\n\\end{split}\n$$\n\nwhere the remaining term $\\Lambda_s \\otimes \\Lambda_t + \\sigma^2 I_m$ is a diagonal matrix, and we can leverage Kronecker matrix-vector (and matrix-matrix) product whenever necessary. \n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}